---
layout: post
title: "ê²½ëŸ‰ ë”¥ëŸ¬ë‹ìœ¼ë¡œ ì‚¬ë¬¼í•¨ ì ìœ  íŒë³„ ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°"
date: 2025-12-24 17:30:00 +0900
description: >
  HOG+SVMì—ì„œ MobileNetV2 ê¸°ë°˜ ë”¥ëŸ¬ë‹ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ê°€ì´ë“œ.
  Transfer Learning, ë°ì´í„° íŒŒì´í”„ë¼ì¸, Fine-tuning, Grad-CAM ì‹œê°í™”ê¹Œì§€
  ì‹¤ì „ êµ¬í˜„ì— í•„ìš”í•œ ëª¨ë“  ë‚´ìš©ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
categories: [42_cabinet]
tags: [deep-learning, cnn, mobilenet, transfer-learning, grad-cam, tensorflow]
---

# ê²½ëŸ‰ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‚¬ë¬¼í•¨ ì ìœ  íŒë³„ êµ¬í˜„ ê°€ì´ë“œ

ë³¸ ë¬¸ì„œëŠ” HOG+SVMì—ì„œ ê²½ëŸ‰ ë”¥ëŸ¬ë‹(MobileNetV2)ìœ¼ë¡œ ì „í™˜í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹ê³¼ êµ¬í˜„ ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ì „ì²´ í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì ìœ  íŒë³„ í”Œë¡œìš°                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1. ì´ë¯¸ì§€ ì…ë ¥]
      â”‚
      â–¼
[2. ì „ì²˜ë¦¬ (Preprocessing)]
      â”‚  - ë¦¬ì‚¬ì´ì¦ˆ (224x224)
      â”‚  - ì •ê·œí™” (0~1 ë˜ëŠ” ImageNet í‰ê· /í‘œì¤€í¸ì°¨)
      â”‚
      â–¼
[3. íŠ¹ì§• ì¶”ì¶œ (Feature Extraction)]
      â”‚  - CNN(MobileNetV2)ì´ ì´ë¯¸ì§€ì—ì„œ ìë™ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
      â”‚  - 1280ì°¨ì›ì˜ íŠ¹ì§• ë²¡í„° ì¶œë ¥
      â”‚
      â–¼
[4. ë¶„ë¥˜ (Classification)]
      â”‚  - Dense Layer + Sigmoid
      â”‚  - 0~1 ì‚¬ì´ í™•ë¥  ì¶œë ¥
      â”‚
      â–¼
[5. ê²°ê³¼ í•´ì„]
      - 0.6 ì´ìƒ: OCCUPIED
      - 0.4 ì´í•˜: EMPTY
      - ì‚¬ì´ê°’: UNKNOWN
```

---

## HOG+SVM vs CNN ë¹„êµ

| í•­ëª©            | HOG+SVM                     | CNN (MobileNetV2)                 |
| --------------- | --------------------------- | --------------------------------- |
| **íŠ¹ì§• ì¶”ì¶œ**   | ìˆ˜ë™ ì„¤ê³„ (ê·¸ë˜ë””ì–¸íŠ¸ ë°©í–¥) | ìë™ í•™ìŠµ (í•„í„° ê°€ì¤‘ì¹˜)           |
| **í•™ìŠµ ëŒ€ìƒ**   | SVM ë¶„ë¥˜ê¸°ë§Œ                | ì „ì²´ ë„¤íŠ¸ì›Œí¬ (ë˜ëŠ” ë¶„ë¥˜ê¸°ë§Œ)     |
| **ì¼ë°˜í™”**      | í•™ìŠµ ë°ì´í„°ì— ê°•í•˜ê²Œ ì˜ì¡´   | Transfer Learningìœ¼ë¡œ ë†’ì€ ì¼ë°˜í™” |
| **ìƒˆë¡œìš´ ë¬¼ì²´** | ì¬í•™ìŠµ í•„ìš”                 | ìƒë‹¹ ë¶€ë¶„ ìë™ ëŒ€ì‘               |

---

## Task 1: Transfer Learning ì´í•´

### í•„ìš”í•œ ì§€ì‹

**Transfer Learning(ì „ì´ í•™ìŠµ)**ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹(ImageNet, 1400ë§Œì¥)ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì˜ ì§€ì‹ì„ ìš°ë¦¬ ë¬¸ì œì— "ì „ì´"í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

#### ì›ë¦¬

```
ImageNet í•™ìŠµëœ MobileNetV2
        â”‚
        â”œâ”€â”€ ì´ˆê¸° ë ˆì´ì–´: ì—£ì§€, ìƒ‰ìƒ, í…ìŠ¤ì²˜ ë“± ë²”ìš© íŠ¹ì§•
        â”‚              (ì´ ë¶€ë¶„ì€ ì¬ì‚¬ìš©)
        â”‚
        â”œâ”€â”€ ì¤‘ê°„ ë ˆì´ì–´: íŒ¨í„´, ë¶€ë¶„ í˜•íƒœ
        â”‚
        â””â”€â”€ ë§ˆì§€ë§‰ ë ˆì´ì–´: ImageNet 1000ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜
                        (ì´ ë¶€ë¶„ë§Œ êµì²´)
```

CNNì˜ ì´ˆê¸° ë ˆì´ì–´ë“¤ì€ **ë²”ìš©ì ì¸ ì‹œê° íŠ¹ì§•**(ì—£ì§€, í…ìŠ¤ì²˜ ë“±)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ íŠ¹ì§•ë“¤ì€ ì–´ë–¤ ì´ë¯¸ì§€ ë¬¸ì œì—ë„ ìœ ìš©í•˜ë¯€ë¡œ, ìš°ë¦¬ëŠ” ì´ ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  **ë§ˆì§€ë§‰ ë¶„ë¥˜ ë¶€ë¶„ë§Œ ìš°ë¦¬ ë¬¸ì œì— ë§ê²Œ êµì²´**í•©ë‹ˆë‹¤.

#### ì™œ ì‘ë™í•˜ëŠ”ê°€?

MobileNetV2ëŠ” ImageNetì—ì„œ ìˆ˜ë°±ë§Œ ê°€ì§€ ë¬¼ì²´ë¥¼ ë³¸ ì ì´ ìˆìŠµë‹ˆë‹¤:

- ì»µ, í…€ë¸”ëŸ¬, ê°€ë°©, ì±…, ìš°ì‚° ë“±
- ë‹¤ì–‘í•œ ì¡°ëª…, ê°ë„, ë°°ê²½

ë”°ë¼ì„œ "ì²˜ìŒ ë³´ëŠ”" í°ìƒ‰ í…€ë¸”ëŸ¬ë„ ì´ë¯¸ ë¹„ìŠ·í•œ íŒ¨í„´ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— ì¸ì‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### êµ¬í˜„ ë°©ì‹

```python
# ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ë¶„ë¥˜ í—¤ë“œ ì œì™¸)
base_model = MobileNetV2(weights='imagenet', include_top=False)

# íŠ¹ì§• ì¶”ì¶œê¸°ë¡œë§Œ ì‚¬ìš© (ê°€ì¤‘ì¹˜ ê³ ì •)
base_model.trainable = False

# ìš°ë¦¬ ë¬¸ì œìš© ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜
])
```

---

## Task 2: MobileNetV2 ì•„í‚¤í…ì²˜ ì´í•´

### í•„ìš”í•œ ì§€ì‹

**MobileNetV2**ëŠ” ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤ìš©ìœ¼ë¡œ ì„¤ê³„ëœ ê²½ëŸ‰ CNNì…ë‹ˆë‹¤.

#### í•µì‹¬ ê°œë…: Depthwise Separable Convolution

ì¼ë°˜ Convolution:

```
ì…ë ¥ â†’ [3x3 Conv, ëª¨ë“  ì±„ë„] â†’ ì¶œë ¥
ì—°ì‚°ëŸ‰: H Ã— W Ã— C_in Ã— C_out Ã— KÂ²
```

Depthwise Separable Convolution:

```
ì…ë ¥ â†’ [3x3 Depthwise Conv, ì±„ë„ë³„] â†’ [1x1 Pointwise Conv] â†’ ì¶œë ¥
ì—°ì‚°ëŸ‰: H Ã— W Ã— C_in Ã— (KÂ² + C_out)
```

**íš¨ê³¼**: ì—°ì‚°ëŸ‰ 8-9ë°° ê°ì†Œ, ì •í™•ë„ëŠ” ë¹„ìŠ·í•˜ê²Œ ìœ ì§€

#### ì™œ MobileNetV2ì¸ê°€?

| ëª¨ë¸            | íŒŒë¼ë¯¸í„° | Top-1 ì •í™•ë„ | ì¶”ë¡  ì‹œê°„ (CPU) |
| --------------- | -------- | ------------ | --------------- |
| VGG16           | 138M     | 71.3%        | ~500ms          |
| ResNet50        | 25M      | 76.0%        | ~100ms          |
| **MobileNetV2** | **3.4M** | 72.0%        | **~50ms**       |

CPUì—ì„œ **ì‹¤ì‹œê°„ ì¶”ë¡ **ì´ ê°€ëŠ¥í•˜ë©´ì„œë„ ì¶©ë¶„í•œ ì •í™•ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

## Task 3: ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### í•„ìš”í•œ ì§€ì‹

ë”¥ëŸ¬ë‹ í•™ìŠµì—ëŠ” íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”©ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

#### ImageDataGenerator vs tf.data

```python
# ë°©ë²• 1: ImageDataGenerator (ê°„ë‹¨)
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    horizontal_flip=True,
    validation_split=0.2
)

train_gen = datagen.flow_from_directory(
    'data/train',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)
```

#### ì „ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­

MobileNetV2ëŠ” íŠ¹ì • ì „ì²˜ë¦¬ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤:

```python
# ImageNet ì •ê·œí™”
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

# ë˜ëŠ” ì§ì ‘ êµ¬í˜„
image = image / 127.5 - 1  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
```

---

## Task 4: ëª¨ë¸ í•™ìŠµ (Fine-tuning)

### í•„ìš”í•œ ì§€ì‹

#### í•™ìŠµ ì „ëµ

**1ë‹¨ê³„: Feature Extraction (ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ)**

```python
base_model.trainable = False
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(train_data, epochs=10)
```

**2ë‹¨ê³„: Fine-tuning (ì¼ë¶€ ë ˆì´ì–´ í•´ë™)**

```python
# ë§ˆì§€ë§‰ 30ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ
base_model.trainable = True
for layer in base_model.layers[:-30]:
    layer.trainable = False

# ë‚®ì€ í•™ìŠµë¥ ë¡œ ì¬í•™ìŠµ
model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy')
model.fit(train_data, epochs=10)
```

#### ì™œ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ„ëŠ”ê°€?

1. **1ë‹¨ê³„**: ëœë¤ ì´ˆê¸°í™”ëœ ë¶„ë¥˜ í—¤ë“œê°€ ì•ˆì •í™”
2. **2ë‹¨ê³„**: ì•ˆì •í™”ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¡œ base ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •

ì²˜ìŒë¶€í„° ì „ì²´ë¥¼ í•™ìŠµí•˜ë©´ **Catastrophic Forgetting**(ê¸°ì¡´ ì§€ì‹ ë§ê°)ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## Task 5: ëª¨ë¸ ë³€í™˜ ë° ìµœì í™”

### í•„ìš”í•œ ì§€ì‹

#### TensorFlow Lite (ì„ íƒì )

ë” ë¹ ë¥¸ CPU ì¶”ë¡ ì„ ìœ„í•´ TFLiteë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # ì–‘ìí™”
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

#### ONNX ë³€í™˜ (PyTorch ì‚¬ìš© ì‹œ)

```python
import torch.onnx

torch.onnx.export(model, dummy_input, "model.onnx")
```

---

## Task 6: ì¶”ë¡  ì½”ë“œ í†µí•©

### í•„ìš”í•œ ì§€ì‹

#### ê¸°ì¡´ API êµ¬ì¡° ìœ ì§€

í˜„ì¬ `extract_features()` í•¨ìˆ˜ë¥¼ CNN ê¸°ë°˜ìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤:

```python
# ê¸°ì¡´ (HOG+SVM)
def extract_features(image, config):
    # HOG ì¶”ì¶œ
    return hog_features

# ë³€ê²½ í›„ (CNN)
def extract_features(image, config):
    # CNN ì „ì²˜ë¦¬
    image = preprocess_input(image)
    # CNN ì¶”ë¡ 
    features = model.predict(image)
    return features
```

**API ì‘ë‹µ í˜•íƒœëŠ” ë™ì¼**í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.

---

## Task 7: ëª¨ë¸ í•´ì„ ì‹œê°í™” (Grad-CAM)

### í•„ìš”í•œ ì§€ì‹

**Grad-CAM (Gradient-weighted Class Activation Mapping)**ì€ CNNì´ **ì–´ë–¤ ì˜ì—­ì„ ë³´ê³  íŒë‹¨í–ˆëŠ”ì§€** ì‹œê°í™”í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

#### ì›ë¦¬

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Grad-CAM ì›ë¦¬                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1. ìˆœì „íŒŒ (Forward)]
    ì´ë¯¸ì§€ â†’ CNN â†’ ì˜ˆì¸¡ í™•ë¥  (ì˜ˆ: occupied 0.85)

[2. ì—­ì „íŒŒ (Backward)]
    ì˜ˆì¸¡ í´ë˜ìŠ¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë§ˆì§€ë§‰ Conv ë ˆì´ì–´ê¹Œì§€ ê³„ì‚°

[3. ê°€ì¤‘í•© (Weighted Sum)]
    ê° ì±„ë„ì˜ ì¤‘ìš”ë„(ê·¸ë˜ë””ì–¸íŠ¸ í‰ê· ) Ã— íŠ¹ì§•ë§µ

[4. í™œì„±í™” (ReLU)]
    ìŒìˆ˜ ì œê±° â†’ íˆíŠ¸ë§µ ìƒì„±

[5. ì˜¤ë²„ë ˆì´]
    íˆíŠ¸ë§µì„ ì›ë³¸ ì´ë¯¸ì§€ì— ê²¹ì³ í‘œì‹œ
```

#### ì™œ í•„ìš”í•œê°€?

1. **ë””ë²„ê¹…**: ëª¨ë¸ì´ ì—‰ëš±í•œ ì˜ì—­ì„ ë³´ê³  ìˆëŠ”ì§€ í™•ì¸
2. **ì‹ ë¢°ì„±**: ì˜ˆì¸¡ ê·¼ê±°ë¥¼ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…
3. **ê°œì„  ë°©í–¥**: ì–´ë–¤ íŠ¹ì§•ì´ ë¶€ì¡±í•œì§€ íŒŒì•…

#### êµ¬í˜„ ë°©ì‹

```python
import tensorflow as tf
import numpy as np
import cv2

def make_gradcam_heatmap(model, image, last_conv_layer_name, pred_index=None):
    """
    Grad-CAM íˆíŠ¸ë§µ ìƒì„±

    Args:
        model: ì „ì²´ ëª¨ë¸
        image: ì „ì²˜ë¦¬ëœ ì…ë ¥ ì´ë¯¸ì§€ (1, 224, 224, 3)
        last_conv_layer_name: ë§ˆì§€ë§‰ Conv ë ˆì´ì–´ ì´ë¦„
        pred_index: ì‹œê°í™”í•  í´ë˜ìŠ¤ ì¸ë±ìŠ¤ (Noneì´ë©´ ì˜ˆì¸¡ í´ë˜ìŠ¤)
    """
    # ë§ˆì§€ë§‰ Conv ë ˆì´ì–´ ì¶œë ¥ê³¼ ìµœì¢… ì˜ˆì¸¡ì„ ì–»ëŠ” ëª¨ë¸
    grad_model = tf.keras.Model(
        inputs=model.input,
        outputs=[
            model.get_layer(last_conv_layer_name).output,
            model.output
        ]
    )

    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    with tf.GradientTape() as tape:
        conv_output, predictions = grad_model(image)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    # ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì¶œ
    grads = tape.gradient(class_channel, conv_output)

    # ì±„ë„ë³„ ì¤‘ìš”ë„ (Global Average Pooling)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # ê°€ì¤‘í•©
    conv_output = conv_output[0]
    heatmap = conv_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # ì •ê·œí™”
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()


def overlay_gradcam(image, heatmap, alpha=0.4):
    """
    íˆíŠ¸ë§µì„ ì›ë³¸ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´

    Args:
        image: ì›ë³¸ ì´ë¯¸ì§€ (H, W, 3)
        heatmap: Grad-CAM íˆíŠ¸ë§µ
        alpha: íˆ¬ëª…ë„
    """
    # íˆíŠ¸ë§µ ë¦¬ì‚¬ì´ì¦ˆ
    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))

    # ì»¬ëŸ¬ë§µ ì ìš©
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    # ì˜¤ë²„ë ˆì´
    superimposed = cv2.addWeighted(image, 1-alpha, heatmap, alpha, 0)
    return superimposed
```

#### ì‹œê°í™” ê²°ê³¼ ì˜ˆì‹œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [ì›ë³¸ ì´ë¯¸ì§€]     [Grad-CAM]      [í•´ì„]                         â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚  â–¡ â–¡    â”‚     â”‚  ğŸ”´ğŸ”´   â”‚     "í…€ë¸”ëŸ¬ ì˜ì—­ì— ì§‘ì¤‘"              â”‚
â”‚  â”‚   â—‹     â”‚     â”‚   ğŸŸ¡    â”‚     â†’ ë¬¼ì²´ë¥¼ ì •í™•íˆ ì¸ì‹             â”‚
â”‚  â”‚         â”‚     â”‚         â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚   OCCUPIED        íˆíŠ¸ë§µ                                          â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚         â”‚     â”‚  ğŸŸ¢ğŸŸ¢   â”‚     "ì„ ë°˜ ë°°ê²½ì— ë¶„ì‚°"               â”‚
â”‚  â”‚         â”‚     â”‚  ğŸŸ¢ğŸŸ¢   â”‚     â†’ ë¬¼ì²´ ì—†ìŒì„ í™•ì¸               â”‚
â”‚  â”‚         â”‚     â”‚         â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚   EMPTY           íˆíŠ¸ë§µ                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ìƒ‰ìƒ ì˜ë¯¸:
  ğŸ”´ ë¹¨ê°•: ë†’ì€ í™œì„±í™” (íŒë‹¨ì— ì¤‘ìš”)
  ğŸŸ¡ ë…¸ë‘: ì¤‘ê°„ í™œì„±í™”
  ğŸŸ¢ ì´ˆë¡: ë‚®ì€ í™œì„±í™”
  ğŸ”µ íŒŒë‘: ê±°ì˜ ë¬´ì‹œ
```

#### ì˜¤ë¥˜ ë¶„ì„ í™œìš©

```python
# ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ ë¶„ì„
if predicted != actual:
    heatmap = make_gradcam_heatmap(model, image, 'Conv_1')  # MobileNetV2 ë§ˆì§€ë§‰ ë ˆì´ì–´
    result = overlay_gradcam(original_image, heatmap)

    # ì €ì¥
    cv2.imwrite(f'debug/misclassified_{filename}.jpg', result)
```

ì´ë¥¼ í†µí•´ **ì™œ ëª¨ë¸ì´ í‹€ë ¸ëŠ”ì§€** ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: í™˜ê²½ ì„¤ì •

- [ ] TensorFlow ë˜ëŠ” PyTorch ì„¤ì¹˜
- [ ] GPU ë¹„í™œì„±í™” í™•ì¸ (CPU ì „ìš©)

### Phase 2: ë°ì´í„° ì¤€ë¹„

- [ ] ê¸°ì¡´ ë°ì´í„° êµ¬ì¡° í™•ì¸ (`data/train/empty`, `data/train/occupied`)
- [ ] ë°ì´í„° ë¡œë” êµ¬í˜„
- [ ] ì¦ê°• íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### Phase 3: ëª¨ë¸ êµ¬ì¶•

- [ ] MobileNetV2 base ëª¨ë¸ ë¡œë“œ
- [ ] ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€
- [ ] ëª¨ë¸ ì»´íŒŒì¼

### Phase 4: í•™ìŠµ

- [ ] 1ë‹¨ê³„: Feature extraction (10 epochs)
- [ ] 2ë‹¨ê³„: Fine-tuning (10 epochs)
- [ ] ê²€ì¦ ì •í™•ë„ í™•ì¸

### Phase 5: í†µí•©

- [ ] ëª¨ë¸ ì €ì¥ (.h5 ë˜ëŠ” SavedModel)
- [ ] `preprocess.py` ìˆ˜ì •
- [ ] `config.py` ëª¨ë¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
- [ ] API í…ŒìŠ¤íŠ¸

### Phase 6: ê²€ì¦ ë° í•´ì„

- [ ] ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ì •í™•ë„ í™•ì¸
- [ ] ì¶”ë¡  ì‹œê°„ ì¸¡ì • (ëª©í‘œ: <100ms)
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] **Grad-CAM ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„**
- [ ] **ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ ë¶„ì„ ë° íˆíŠ¸ë§µ ì €ì¥**
- [ ] **ëª¨ë¸ íŒë‹¨ ê·¼ê±° ê²€ì¦**

---

## ì˜ˆìƒ ê²°ê³¼

| í•­ëª©                     | HOG+SVM  | CNN (MobileNetV2) |
| ------------------------ | -------- | ----------------- |
| í•™ìŠµ ì‹œê°„                | ~1ë¶„     | ~30ë¶„-2ì‹œê°„ (CPU) |
| ëª¨ë¸ í¬ê¸°                | ~1MB     | ~10-15MB          |
| ì¶”ë¡  ì‹œê°„                | ~5ms     | ~50-100ms         |
| ì •í™•ë„ (í•™ìŠµ ë°ì´í„°)     | ~95%     | ~95-99%           |
| **ì¼ë°˜í™” (ìƒˆë¡œìš´ ë¬¼ì²´)** | **ë‚®ìŒ** | **ë†’ìŒ**          |

---

## ì°¸ê³  ìë£Œ

- [TensorFlow Transfer Learning Guide](https://www.tensorflow.org/tutorials/images/transfer_learning)
- [MobileNetV2 Paper](https://arxiv.org/abs/1801.04381)
- [Keras Applications](https://keras.io/api/applications/)
